# Bachelor-Project-SlideWiki
This is the implementation of the recommendation system used, along with the different models that were applied for spam classification on the dataset which is based of SlideWiki.org. Everything that is needed to reproduce the results in the thesis is present. 

There are some files that are zipped in .7z files. These are optional, but they have to be unpacked if you wish to see a quick demonstration of e.g. the recommendation system.

This document will further explain the inidividual files present in this project.


# scrapings.py and processing.py: scraping and preprocessing
This is the first step to the implementation: gathering the data and preprocessing it. The decks.7z file in the root is the result of scraping SlideWiki and preprocessing of the data.

The recommendation system or any classifier in fact needs a dataset to train on. In this case the dataset consists of SlideWiki’s decks in plain text. The entire collection of SlideWiki’s decks was scraped on the 28th of May 2018 and 27012 decks were retrieved. The getDeckId function on SlideWiki’s own deckservice (https://deckservice.slidewiki.org/documentation#!/deck/getDeckId) was repurposed in Python to scrape all the decks that were available on SlideWiki at that time. Since SlideWiki uses simple integer value for their deck IDs, a for loop of 0 to 999999 was used to check for every value whether it was an existing deck ID. A response code of 200 meant that the deck with the given deck ID i exists and it could thus be scraped. As the deck is formatted in HTML, the first step was to only get the text from the response. LXML proved to be very helpful with that. The last step was retrieve the plain text content of the deck by searching for text that fell within a defined regex pattern. 

While plain text is a good start regarding data to process, the dataset could still use some pre-processing. The initial dataset consists of decks written in various languages (i.e. Hindi, Russian, Greek et cetera) and there is also a rather large proportion of very short decks. These shorter decks were first of all mostly decks that were made for testing purposes and furthermore, other non-test decks that are of such small size will not be informative anyway. The first course of action on the initial dataset was filter-ing out any decks where its plain text content was smaller than 1kB. This resulted in a dataset of 20460 decks 
After removing the small decks, the contents still require further processing. Non-Latin symbols, punctuation marks and stopwords were removed. Furthermore, all content was turned into lowercase text and lemmatization from NLTK’s lemmatiza-tion library was applied to the decks as the final step. Lemmatization is particularly useful for this use case, as it tries to unify all inflicted forms of a term into a single word. This prevents information loss and it also prevents the likelihood of the inflict-ed forms of a term (car, cars, …) to be split over multiple classes in the model space of our classifiers. 

Trying to keep the scope of this project somewhat narrow, non-English decks were removed as well. Non-English decks were filtered out using a Python library, called langdetect. This library is a port of  a Google library and it has the function of esti-mating the possible language(s) of a given input string. Filtering non-English decks with the help of langdetect resulted in a total of 16723 likely English slides. One should however keep in mind of that of course that the filtering process was auto-mated process where the program merely estimates the likelihood that a deck i is either English or not. Given that this is not a perfect world and despite all optimiza-tions made, the "English-only" classifier is not perfect either. This does mean that errors did occur where non-English decks were overlooked by the classifier and actual English decks were unjustly filtered out as being considered non-English by the classi-fier. Regarding the classifier’s efficacy, precision and recall was measured. 200 ran-dom samples were retrieved, 100 from the resulting English set of decks and 100 from the resulting non-English set of decks. The classifier has a recall of 0.81 and precision of 0.92. The fact that the dataset is imperfect (i.e. not completely “English-only”) should be kept in mind when commenting on the efficacy of both the spam filters and the recommendation systems.

After pre-processing the data is in principle ready to be processed through the LDA topic modelling classifier. However from the set of 16723 decks, there is still a large prevalence of spam present in the dataset. Three different models have been consid-ered for this project and they will later on be compared in terms of performance in one of the experiments. Regarding the final sets, there are four of them. 
  •	Naïve Bayes spam filtering resulted in 4476 decks, 
  •	LDA spam filtering resulted in 3948 decks
  •	Spam filtering by the ensemble resulted in 3781 decks.
  

# naive_bayes_trainer.py and naive_bayes_spamfilter.py: Naïve Bayes spam classification
The implementations of the Naïve Bayes classifier can be found in the bayes and ensemble folders. Both models have already been trained and they have already performed classification on the English-only dataset of SlideWiki's decks. The settings of the trained NB models can be found in the pickles.7z files, which are present in both the bayes and ensemble folders. Keep in mind however that the pickles.7z in the ensemble folder also contains the files for the trained LDA model.

The initial idea was to just tackle the spam head-on and build a Naïve Bayes spam filter. Naïve Bayes classification was the first solution that came to mind when faced with the large presence of spam in the SlideWiki dataset, since the textbook example of Naïve Bayes being applied is it being trained and implemented as spam filter. The implementation of the Naïve Bayes spam filter is largely based on an example that can be found on Cambridge Spark. (https://cambridgespark.com/content/tutorials/implementing-your-own-spam-filter/index.html) The article by Ekaterina Kochmar shows how a spam filter based on Naïve Bayes classification can be built using Python’s NLTK package. 
Aside from its associations as being a spam filter, Naïve Bayes has some other characteristics that make it interesting in this particular problem.	A Naïve Bayes spam classifier is relatively easy to train. Give it a set of labelled examples of both classes “ham” and “spam.” Train the model on this labelled training set such that it learns what documents should be considered spam and which documents should not be considered as spam based on the examples provided in the training set. In this case, the resulting labelled set consisted of 1151 “ham” labelled examples and 3808 “spam” labelled examples. 85% of this set is used for training, while the remaining 15% was set aside as a test set to evaluate the Naïve Bayes spam filter. This resulted in the following sets:
•	978 ham training
•	173 ham test
•	3236 spam training
•	571 spam test

Another upside to using Naïve Bayes classification is the relatively high speed when performing classification. The spam filter is able to process the 17K set of  Eng-lish decks in 11 minutes and 5 seconds. 
A downside with Naïve Bayes however is the performance of the spam filter itself. The classifier has an accuracy of 85% on the test set. This is by no means bad, but it is far from perfect. Furthermore, the evaluation on the test set only returned the accu-racy. Its precision and recall on the test set is unknown. A high recall is of course de-sirable, since it would mean that the classifier is sensitive enough to filter out a large proportion of spam, however precision is arguably much more important. When evaluation a spam classifier’s efficacy in terms of costs of the resulting errors, it be-comes clear why precision is much more important than recall. In the case of a false negative, the worst what can happen is that a spam document is given as a recom-mendation by the recommendation system. This would be mildly infuriating at most, while a false positive results in losing a non-spam document, as it was incorrectly classified as spam. With a non-spam SlideWiki deck it would mean that for example someone’s lecture on Artificial Intelligence got filtered away. That is valuable infor-mation that is lost due to an incorrect classification. 
 Furthermore, Naïve-Bayes uses supervised learning. The spam filter is constructed by providing labelled examples that are spam and not spam. Unless the sample sets are constantly updated as new decks are uploaded, the spam filter risks to become worse as the website grows in more and more distinct content. Especially when con-sidering that SlideWiki aspires to be a platform to create and share decks about any subject, worse performance of this spam filter over time seems inevitable.

